{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define network parameters\n",
    "max_features = 20000\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import Conv1D, GlobalMaxPool1D, Dropout, concatenate\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"Invalid\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "# train data\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(conv_layers = 2, max_dilation_rate = 4):\n",
    "    embed_size = 128\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv1D(2*embed_size, \n",
    "                   kernel_size = 3)(x)\n",
    "    prefilt_x = Conv1D(2*embed_size, \n",
    "                   kernel_size = 3)(x)\n",
    "    out_conv = []\n",
    "    # dilation rate lets us use ngrams and skip grams to process \n",
    "    for dilation_rate in range(max_dilation_rate):\n",
    "        x = prefilt_x\n",
    "        for i in range(3):\n",
    "            x = Conv1D(32*2**(i), \n",
    "                       kernel_size = 3, \n",
    "                       dilation_rate = 2**dilation_rate)(x)    \n",
    "        out_conv += [Dropout(0.5)(GlobalMaxPool1D()(x))]\n",
    "    x = concatenate(out_conv, axis = -1)    \n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['binary_accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 15\n",
    "\n",
    "file_path=\"weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "model.fit(X_t, y, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          validation_split=0.1, \n",
    "          callbacks=callbacks_list)\n",
    "model.load_weights(file_path)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_results = model.evaluate(X_t, y, batch_size=batch_size)\n",
    "for c_name, c_val in zip(model.metrics_names, eval_results):\n",
    "    print(c_name, '%2.3f' % (c_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import dask.dataframe as ddf\n",
    "rustweet_dir = os.path.join('..', 'input', 'russian-troll-tweets')\n",
    "all_tweets_ddf = ddf.read_csv(os.path.join(rustweet_dir, '*.csv'), assume_missing=True)\n",
    "english_tweets_ddf = all_tweets_ddf[all_tweets_ddf['language'].isin(['English'])]\n",
    "content_cat_ddf = english_tweets_ddf[['content', 'account_category']]\n",
    "all_tweets_ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "content_cat_df = content_cat_ddf.sample(frac=0.2).compute().drop_duplicates()\n",
    "print(content_cat_df.shape[0], 'tweets loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1, figsize = (15, 5))\n",
    "content_cat_df['account_category'].hist(ax=ax1)\n",
    "content_cat_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test data\n",
    "list_tweets = content_cat_df[\"content\"].fillna(\"Invalid\").values\n",
    "list_tokenized_tweets = tokenizer.texts_to_sequences(list_tweets)\n",
    "X_twe = sequence.pad_sequences(list_tokenized_tweets, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run the model on all data\n",
    "y_twe = model.predict(X_twe, batch_size=1024, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toxicity_df = pd.DataFrame(y_twe, columns = list_classes)\n",
    "toxicity_df['content_category'] = content_cat_df['account_category'].values.copy()\n",
    "toxicity_df['total_hatefulness'] = np.sum(y_twe, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(toxicity_df, hue = 'content_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display_markdown = lambda x: display(Markdown(x))\n",
    "def show_sentence(sent_idx):\n",
    "    display_markdown('# Input Sentence:\\n `{}`'.format(list_tweets[sent_idx]))\n",
    "    c_pred = model.predict(X_twe[sent_idx:sent_idx+1])[0]\n",
    "    display_markdown('## Scores')\n",
    "    for k, p in zip(list_classes, c_pred):\n",
    "        display_markdown('- {}, Prediction: {:2.2f}%'.format(k, 100*p))\n",
    "show_sentence(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "worst_tweets = np.argsort(-1*toxicity_df['total_hatefulness'].values)\n",
    "for _, idx in zip(range(5), \n",
    "                  worst_tweets):\n",
    "    show_sentence(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toxicity_df.groupby('content_category').agg(lambda x: round(100*np.mean(x))).reset_index().sort_values('total_hatefulness', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toxicity_df.groupby('content_category').agg(lambda x: round(100*np.max(x))).reset_index().sort_values('total_hatefulness', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_sample_df = toxicity_df.groupby('content_category').apply(lambda x: x.sample(250, replace=False if x.shape[0]>1000 else True)).reset_index(drop=True)\n",
    "sns.factorplot(y='content_category', x='identity_hate', kind='swarm', data=cat_sample_df, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rescale the axes a bit\n",
    "clip_tox_df = toxicity_df.copy()\n",
    "for c_class in list_classes:\n",
    "    clip_tox_df[c_class] = np.sqrt(np.clip(clip_tox_df[c_class], 0, .025))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.factorplot(y='content_category', x='identity_hate', kind='violin', data=clip_tox_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.factorplot(y='content_category', x='threat', kind='violin', data=clip_tox_df, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "tx_train_df, tx_valid_df = train_test_split(toxicity_df, \n",
    "                                            test_size = 0.25,\n",
    "                                            random_state = 2018,\n",
    "                                            stratify=toxicity_df['content_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.dummy import DummyClassifier\n",
    "dmc = DummyClassifier()\n",
    "def fit_and_show(in_skl_model):\n",
    "    in_skl_model.fit(tx_train_df[list_classes], tx_train_df['content_category'])\n",
    "    out_pred = in_skl_model.predict(tx_valid_df[list_classes])\n",
    "    print('%2.2f%%' % (100*accuracy_score(out_pred, tx_valid_df['content_category'])), 'accuracy')\n",
    "    print(classification_report(out_pred, tx_valid_df['content_category']))\n",
    "    sns.heatmap(confusion_matrix(tx_valid_df['content_category'], out_pred))\n",
    "fit_and_show(dmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lrm = LogisticRegression()\n",
    "fit_and_show(lrm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "fit_and_show(rfc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
